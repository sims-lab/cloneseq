"""===========================
Pipeline template
===========================

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_cloneseq.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""

import sys
import os
import ruffus
import pandas
import pysam

from CGATCore import IOTools as IOTools
from CGATCore import Pipeline as P


def index_vector_sequence(infile, outfile):
    statement = (
        "cp {infile} {outfile} && "
        "samtools faidx {outfile} && "
        "bwa index {outfile} "
        "> {outfile}.log ".format(**locals()))
    return P.run(statement)


def build_motif_bed(infile, outfile):
    
    with pysam.FastxFile(infile) as inf:
        vector = next(inf)

    contig = vector.name
    positions = [x for x, c in enumerate(vector.sequence) if c == "N"]

    with IOTools.open_file(outfile, "w") as outf:
        for pos in positions:
            outf.write("{}\t{}\t{}\n".format(contig, pos, pos + 1))


def filter_read_data(infiles, outfiles):
    """filter vector sequences from read data.
    """

    fastq1, fastq2 = sorted([infiles[0][0] for x in infiles])
    vector_fasta = infiles[0][1]
    outprefix = IOTools.snip(outfiles[0], ".matched.fastq.1.gz")
    
    statement = (
        "cgat fastqs2fastqs "
        "--input-filename-fasta={vector_fasta} "
        "--method=filter-by-sequence "
        "--force-output "
        "--output-filename-pattern={outprefix}.%%s "
        "{fastq1} "
        "{fastq2} "
        "> {outprefix}.log ".format(
            **locals()))

    return P.run(statement)


def summarize_filtering(infiles, outfile):

    tables = " ".join([P.snip(x[0], ".matched.fastq.1.gz") + ".log" for x in infiles])
    statement = (
        "cgat combine-tables "
        "--log={outfile}.log "
        "--cat sample "
        "--regex-filename=\".dir/([^/.]+)\" "
        "{tables} "
        "> {outfile}".format(**locals()))
    return P.run(statement)


def align_vector_sequences(infiles, outfile):

    fastq = " ".join(sorted([x for x in infiles[0][0] if ".matched" in x]))
    vector_fasta = infiles[0][1]

    statement = (
        "bwa mem -k14 -W20 -r10 -A1 -B1 -O6 -E1 -L0 -U0 "
        "{vector_fasta} {fastq} "
        "2> {outfile}.bwa.err "
        "| samtools sort - "
        "2> {outfile}.sort.err "
        "| samtools view -bS "
        "2> {outfile}.view.err "
        "> {outfile} && "
        "samtools index {outfile}".format(
            **locals()))
        
    return P.run(statement)


def merge_lanes(infiles, outfile):
    
    infiles = " ".join(infiles)
    statement = (
        "samtools merge {outfile} {infiles} "
        "2> {outfile}.merge.err && "
        "samtools index {outfile} ".format(**locals()))
    return P.run(statement)


def compute_samtools_bamstats(infile, outfile):
    statement = (
        "samtools stats {infile} "
        "> {outfile} ".format(**locals()))
    return P.run(statement)


def compute_samtools_depth(infile, outfile):
    statement = (
        "echo -e \"contig\\tpos\\tdepth\" > {outfile} && "
        "samtools depth {infile} "
        ">> {outfile} ".format(**locals()))
    return P.run(statement)


def summarize_samtools_depth(infiles, outfile):
    infiles = " ".join(infiles)
    statement = (
        "cgat combine-tables "
        "--log={outfile}.log "
        "--cat sample "
        "--regex-filename=\".dir/([^/.]+)\" "
        "{infiles} "
        "> {outfile}".format(**locals()))
    return P.run(statement)


def compute_cgat_bamstats(infile, outfile):
    statement = (
        "cgat bam2stats {infile} "
        "--force-output "
        "--output-filename-pattern={outfile}.%%s "
        "--log={outfile}.log "
        "> {outfile} ".format(**locals()))
    return P.run(statement)


def summarize_cgat_bamstats(infiles, outfile):

    infiles = " ".join(infiles)
    statement = (
        "cgat combine-tables "
        "--log={outfile}.log "
        "--cat sample "
        "--regex-filename=\".dir/([^/.]+)\" "
        "{infiles} "
        "> {outfile}".format(**locals()))
    return P.run(statement)


def extract_clone_codes(infiles, outfile):

    bamfile, fafile, bedfile = infiles

    # -A: count orphans (do not discard anomolous read pairs)
    # -a: output zero depth positions
    statement = (
        "echo -e \"contig\\tpos\\tref\\tdepth\\tbases\\tquality\" > {outfile}.tmp && "
        "samtools mpileup -a -A "
        "--fasta-ref {fafile} "
        "--positions {bedfile} "
        "{bamfile} "
        ">> {outfile}.tmp ".format(**locals()))
    P.run(statement)

    df = pandas.read_csv(outfile + ".tmp", sep="\t")
    os.unlink(outfile + ".tmp")
    
    bases = ["A", "C", "G", "T"]
    for b in bases:
        df[b] = df.bases.str.upper().str.count(b)
    df["consensus"] = df[bases].idxmax(axis=1)
    df["consensus_counts"] = df.lookup(df.index, df.consensus)
    df["consensus_support"] = df.consensus_counts / df.depth
    df["offconsensus_counts"] = df.depth - df.consensus_counts
    df.loc[df.consensus_counts == 0, "consensus"] = "N"

    with IOTools.open_file(outfile + ".pileup", "w") as outf:
        df.to_csv(outf, sep="\t", index=False)
    
    with IOTools.open_file(outfile, "w") as outf:
        headers = df.consensus_support.describe().index
        outf.write("\t".join(map(str, ["barcode"] +
                                 ["support_{}".format(x) for x in headers] +
                                 ["counts_{}".format(x) for x in headers] +
                                 ["offcounts_{}".format(x) for x in headers])) + "\n")

        outf.write("\t".join(map(str, ["".join(df.consensus)] +
                                 df.consensus_support.describe().tolist() +
                                 df.consensus_counts.describe().tolist() +
                                 df.offconsensus_counts.describe().tolist())) + "\n")


def summarize_clone_codes(infiles, outfile):
            
    infiles = " ".join(infiles)
    statement = (
        "cgat combine-tables "
        "--log={outfile}.log "
        "--cat sample "
        "--regex-filename=\".dir/([^/.]+)\" "
        "{infiles} "
        "> {outfile}".format(**locals()))
    return P.run(statement)


# todo: depth profiles

def main(argv=None):
    if argv is None:
        argv = sys.argv

    def add_input(parser):
        parser.add_option(
            "--filename-vector-fasta",
            dest="filename_vector_fasta",
            default=None,
            type="string",
            help="filename of vector sequence in fasta format "
            "[default=%default].")

        parser.add_option(
            "--input-fastq-glob",
            dest="input_fastq_glob",
            default=None,
            type="string",
            help="glob expression for paired-end read data in fastq format "
            "[default=%default].")

    options, args = P.parse_commandline(argv,
                                        config_file="pipeline.yml",
                                        callback=add_input)

    if options.filename_vector_fasta is None:
        raise ValueError("please specify --vector-fasta")

    if options.input_fastq_glob is None:
        raise ValueError("please specify --input-fastq-glob")

    if options.config_file:
        P.get_parameters(options.config_file)
    else:
        sys.exit(P.main(options, args))

    pipeline = ruffus.Pipeline("cgatflow-cloneseq")

    task_index_vector_sequence = pipeline.merge(
        task_func=index_vector_sequence,
        input=options.filename_vector_fasta,
        output="vector.dir/vector.fa")

    task_build_motif_bed = pipeline.transform(
        task_func=build_motif_bed,
        input=task_index_vector_sequence,
        filter=ruffus.suffix(".fa"),
        output=".bed")

    task_filter_read_data = pipeline.collate(
        task_func=filter_read_data,
        input=options.input_fastq_glob,
        filter=ruffus.formatter(r"(?P<SAMPLE>[^/]+).fastq.[12].gz"),
        output=["fastq.dir/{SAMPLE[0]}.matched.fastq.1.gz",
                "fastq.dir/{SAMPLE[0]}.matched.fastq.2.gz",
                "fastq.dir/{SAMPLE[0]}.unmatched.fastq.1.gz",
                "fastq.dir/{SAMPLE[0]}.unmatched.fastq.2.gz"],
        add_inputs=(task_index_vector_sequence,),
        ).mkdir("fastq.dir")

    task_summarize_filtering = pipeline.merge(
        task_func=summarize_filtering,
        input=task_filter_read_data,
        output="filtering_summary.tsv")
    
    task_align_vector_sequences = pipeline.collate(
        task_func=align_vector_sequences,
        input=task_filter_read_data,
        filter=ruffus.formatter(
            r"fastq.dir/(?P<SAMPLE>[^/]+).matched.fastq.[12].gz"),
        output="mapped.dir/{SAMPLE[0]}.bam",
        add_inputs=(task_index_vector_sequence,),
        ).mkdir("mapped.dir")

    task_merge_lanes = pipeline.collate(
        task_func=merge_lanes,
        input=task_align_vector_sequences,
        filter=ruffus.formatter(r"mapped.dir/(?P<SAMPLE>[^/]+)-lane(\S+).bam"),
        output="merged_bam.dir/{SAMPLE[0]}.bam",
        ).mkdir("merged_bam.dir")

    task_compute_cgat_bamstats = pipeline.transform(
        task_func=compute_cgat_bamstats,
        input=[task_align_vector_sequences, task_merge_lanes],
        filter=ruffus.formatter(r".dir/(?P<SAMPLE>[^/]+).bam"),
        output="cgat_bamstats.dir/{SAMPLE[0]}.tsv",
        ).mkdir("cgat_bamstats.dir")

    task_summarize_cgat_bamstats = pipeline.merge(
        task_func=summarize_cgat_bamstats,
        input=task_compute_cgat_bamstats,
        output="cgat_bamstats.tsv")
    
    task_compute_samtools_bamstats = pipeline.transform(
        task_func=compute_samtools_bamstats,
        input=[task_align_vector_sequences, task_merge_lanes],
        filter=ruffus.formatter(r".dir/(?P<SAMPLE>[^/]+).bam"),
        output="samtools_bamstats.dir/{SAMPLE[0]}.tsv",
        ).mkdir("samtools_bamstats.dir")

    task_compute_samtools_depth = pipeline.transform(
        task_func=compute_samtools_depth,
        input=[task_align_vector_sequences, task_merge_lanes],
        filter=ruffus.formatter(r".dir/(?P<SAMPLE>[^/]+).bam"),
        output="samtools_depth.dir/{SAMPLE[0]}.tsv",
        ).mkdir("samtools_depth.dir")

    task_summarize_samtools_depth = pipeline.merge(
        task_func=summarize_samtools_depth,
        input=task_compute_samtools_depth,
        output="samtools_depth.tsv")
    
    task_extract_clone_codes = pipeline.transform(
        task_func=extract_clone_codes,
        input=[task_align_vector_sequences, task_merge_lanes],
        filter=ruffus.regex(r".dir/([^/]+).bam"),
        output=r"clone_codes.dir/\1.tsv",
        add_inputs=(task_index_vector_sequence, task_build_motif_bed),
        ).mkdir("clone_codes.dir")

    task_summarize_clone_codes = pipeline.merge(
        task_func=summarize_clone_codes,
        input=task_extract_clone_codes,
        output="clone_codes.tsv")

    # primary targets
    pipeline.merge(
        task_func=P.EmptyRunner("all"),
        input=[
            task_merge_lanes,
            task_summarize_filtering,
            task_summarize_cgat_bamstats,
            task_compute_samtools_bamstats,
            task_summarize_samtools_depth,
            task_summarize_clone_codes
        ],
        output="all")

    return P.run_workflow(options, args)


if __name__ == "__main__":
    sys.exit(main(sys.argv))
